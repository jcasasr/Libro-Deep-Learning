# Deep learning: Principios y fundamentos
## Material adicional del libro "**Deep learning: Principios y fundamentos**"
### Por Anna Bosch Rué, Jordi Casas Roma y Toni Lozano Bagén

<a href="https://www.editorialuoc.cat/deep-learning"><img src="https://www.editorialuoc.cat/media/cache/05/16/0516c673ca010fe373fe5a92f0dfa068.jpg" alt="Deep learning: Principios y fundamentos" height="256px" align="right"></a>

**Este github contiene el código del libro [Deep learning: Principios y fundamentos](https://www.editorialuoc.cat/deep-learning), publicado por Editorial UOC**

En esta página encontraréis ejercicios prácticos en [Jupyter](http://jupyter.org/) que os permitirán trabajar más a fondo los contenidos aprendidos en el libro [Deep learning: Principios y fundamentos](https://www.editorialuoc.cat/deep-learning).

La mejor forma de trabajar estos ejercicios es seguir los capítulos del libro, descargarse los archivos de esta página y abrirlos desde un navegador web accediendo a tu instalación Jupyter.

## Introducción
Este libro es el resultado de 3 años de docencia en *machine learning* y *deep learning* dentro del [Máster Universitario en Ciencia de datos (Data Science)](https://estudios.uoc.edu/es/masters-universitarios/data-science/presentacion) de la UOC, a través de los cuales hemos reflexionado junto con nuestros alumnos, la mejor manera de aproximarse al mundo del aprendizaje profundo.

## Ejecución de los ejemplos
Recomendamos los siguientes enlaces para poder instalar los componentes de software necesarios para seguir los ejercicios propuestos:

- [Anaconda](https://www.anaconda.com/products/individual)
- [Entorno de trabajo Jupyter](http://jupyter.org/install.html)

Aunque es posible instalar el software necesario para poder seguir los ejemplos, recomendamos el uso de plataformas gratuitas en la nube (*cloud*) que permiten la ejecución de estos ejemplos de forma mucho más rápida, ya que permiten el uso de GPUs para el entrenamiento de los modelos. Entre múltiples plataformas, recomendamos algunas opciones interesantes:

- [Kaggle notebooks](https://www.kaggle.com/)
- [Google Colab](https://colab.research.google.com/)

# Parte I: Redes neuronales artificiales

## Capítulo 3. Principios y fundamentos
En este capítulo veremos ejemplos básicos de funcionamiento de redes neuronales complemtamente conectadas (feed-forward neural networks).

Ejemplos prácticos de este capítulo:

- [Ejemplo de NN básico](cap03/C03_E1_Ejemplo-NN.ipynb): Ejemplo de redes neuronales completamente conectadas empleando Keras.

## Capítulo 5. *Autoencoders*
En este capítulo veremos ejemplos básicos de funcionamiento de los *autoencoders*.

Ejemplos prácticos de este capítulo:

- [Ejemplo de autoencoder](cap05/C05_E1_Shallow-AutoEncoder.ipynb): Ejemplo de redes neuronales completamente conectadas para crear un *autoencoder*.

# Parte II: Redes neuronales convolucionales

## Capítulo 7. Principios y fundamentos
En este capítulo veremos ejemplos básicos de funcionamiento de redes neuronales convolucionales (CNN).

Ejemplos prácticos de este capítulo:

- [Ejemplo de CNN básico con MNIST](cap07/C07_E1_Ejemplo-CNN-MNIST-Keras.ipynb): Ejemplo básico de red neuronal convolucional empleando Keras y trabajando con el dataset de dígitos MNIST.

## Capítulo 9. Consejos prácticos y ejemplos
En este capítulo veremos algunas aplicaciones y *best practices* de funcionamiento de redes neuronales convolucionales.

Ejemplos prácticos de este capítulo:

- [Ejemplo de transferencia de estilo (NST)](cap09/C09_E1_Ejemplo-Neural-Style-Transfer.ipynb): Ejemplo de uso de una red neuronal para el problema de la transferencia de estilo (Neural Style Transfer, NST).

# Parte III: Redes neuronales recurrentes

## Capítulo 10. Fundamentos de las redes recurrentes
En este capítulo veremos ejemplos básicos de funcionamiento de redes neuronales recurrentes (RNN).

Ejemplos prácticos de este capítulo:

- [Ejemplo de RNN básico aplicado a TSA](cap10/C10_E1_Ejemplo-series-temporales-RNN.ipynb): Ejemplo básico de red neuronal recurrente empleando Keras y aplicado a un problema de análisis de series temporales (TSA).
